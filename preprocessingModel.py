# -*- coding: utf-8 -*-
"""Omblero_Lab Activity 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WriCtKGJbqas9G5xBfUXfavMBNi_XQmD
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Mount Google Drive (if using Google Colab)
from google.colab import drive
drive.mount('/content/drive')

# Read the dataset
nfl_data = pd.read_csv("/content/LoanData_Raw_v1.0.csv")

# Set seed for reproducibility
np.random.seed(0)

# Display first few rows
print(nfl_data.head())

# Get the number of missing data points per column
missing_values_count = nfl_data.isnull().sum()

# Display missing values count for first ten columns
print(missing_values_count[0:10])

# Calculate total missing values
total_cells = np.product(nfl_data.shape)
total_missing = missing_values_count.sum()

# Calculate and print the percentage of missing data
percent_missing = (total_missing / total_cells) * 100
print(f"Percentage of missing data: {percent_missing:.2f}%")

# Fill missing values with the mean of their respective columns
nfl_data.fillna(nfl_data.mean(numeric_only=True), inplace=True)

# Apply StandardScaler to numerical columns
scaler = StandardScaler()
numeric_cols = nfl_data.select_dtypes(include=['number']).columns
nfl_data[numeric_cols] = scaler.fit_transform(nfl_data[numeric_cols])

# Save the cleaned and scaled data to a new CSV file
cleaned_file_path = "/content/LoanData_Cleaned_Scaled.csv"
nfl_data.to_csv(cleaned_file_path, index=False)

print(f"Cleaned and scaled data saved as '{cleaned_file_path}'. You can now download it.")

